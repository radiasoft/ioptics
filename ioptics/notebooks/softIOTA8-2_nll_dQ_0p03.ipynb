{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Non-Linear Element in dQ=-0.1 Adjusted Lattice\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Parameters for SC Match**\n",
    "- 20 mm-mrad emittance - KV distribution\n",
    "- dQ = -0.03\n",
    "- t = 0.1\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import scipy\n",
    "from scipy import constants\n",
    "from scipy.interpolate import interp1d\n",
    "import tables\n",
    "from mpi4py import MPI\n",
    "\n",
    "sys.path.append('/home/vagrant/jupyter/rsbeams/synergia/') #added specifically for nifak.radiasoft.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named base_diagnostics",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b64a9ed40cb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrssynergia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstandard_beam6d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrssynergia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melliptic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0melliptic_beam6d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrssynergia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemigaussian\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msemi_gaussian6d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#from standard import StandardBeam6D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msynergia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vagrant/.pyenv/versions/2.7.10/lib/python2.7/site-packages/rssynergia/semigaussian/semi_gaussian6d.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msemi_gaussian\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSemiGaussianBeam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbase_diagnostics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_bunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbase_diagnostics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mworkflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbase_diagnostics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlatticework\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named base_diagnostics"
     ]
    }
   ],
   "source": [
    "import rssynergia\n",
    "from rssynergia.base_diagnostics import utils\n",
    "from rssynergia.base_diagnostics import read_bunch\n",
    "from rssynergia.base_diagnostics import workflow\n",
    "from rssynergia.base_diagnostics import lfplot\n",
    "from rssynergia.base_diagnostics import latticework\n",
    "from rssynergia.base_diagnostics import basic_calcs\n",
    "from rssynergia.base_diagnostics import pltbunch\n",
    "from rssynergia.base_diagnostics import elliptic_sp\n",
    "from rssynergia.base_diagnostics import singleparticle\n",
    "from rssynergia.base_diagnostics import options\n",
    "from rssynergia.standard import standard_beam6d\n",
    "from rssynergia.elliptic import elliptic_beam6d\n",
    "from rssynergia.semigaussian import semi_gaussian6d\n",
    "#from standard import StandardBeam6D\n",
    "import synergia\n",
    "import synergia_workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load options and lattices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'synergia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8c6886b61a25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#if True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# this is the communicator object that will be used for MPI operations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcomm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynergia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCommxx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmyrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmpisize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'synergia' is not defined"
     ]
    }
   ],
   "source": [
    "#load options for SC_test\n",
    "from SC_test_options import opts\n",
    "\n",
    "#================== Setting up logger and MPI comunicator ============================\n",
    "#try:\n",
    "#if True:\n",
    "# this is the communicator object that will be used for MPI operations\n",
    "comm = synergia.utils.Commxx()\n",
    "myrank = comm.get_rank()\n",
    "mpisize = comm.get_size()\n",
    "verbose = opts.verbosity>0\n",
    "\n",
    "logger = synergia.utils.Logger(0)\n",
    "\n",
    "\n",
    "if myrank == 0:\n",
    "    print \"my rank is 0\"\n",
    "else:\n",
    "    print \"not rank 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#================== Load the lattice =======================\n",
    "\n",
    "lattices = {}\n",
    "dir_66 = '/home/vagrant/jupyter/beamsim/synergia/lattices/Iota6-6/'\n",
    "dir_82 = '/home/vagrant/jupyter/beamsim/synergia/lattices/Iota8-2/'\n",
    "\n",
    "lattices['t1_dQ03_1IO_82'] =\"soft_lattice_1IO_dQ_03.madx\" #t1 8.2 lattice adjusted for dQ = -0.1\n",
    "lattices['t3_dQ03_1IO_82'] =\"soft_lattice_nll_1IO_dQ_03.madx\" #t1 8.2 lattice adjusted for dQ = -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'synergia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5b722f81de5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlattice_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlattice_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'location'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlattices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlattice_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lattice'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynergia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlattice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMadX_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lattice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iota\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlattices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mlatticework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_lattice_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlattice_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lattice'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'synergia' is not defined"
     ]
    }
   ],
   "source": [
    "#================= Construct a Python dictionary of lattice stuff ==================\n",
    "lattice_dict = {}\n",
    "\n",
    "for keys in lattices.keys():\n",
    "    lattice_dict[keys] = {} #instantiate sub dictionary\n",
    "    lattice_dict[keys]['name'] = keys\n",
    "    lattice_dict[keys]['location'] = lattices[keys]\n",
    "    lattice_dict[keys]['lattice'] = synergia.lattice.MadX_reader().get_lattice(\"iota\", lattices[keys])\n",
    "    latticework.set_lattice_element_type(lattice_dict[keys]['lattice'],opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lattice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-10272bea32a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreference_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlattice_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't1_dQ03_1IO_82'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lattice'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reference_particle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreference_particle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_total_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreference_particle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_beta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreference_particle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lattice'"
     ]
    }
   ],
   "source": [
    "reference_particle = lattice_dict['t1_dQ03_1IO_82']['lattice'].get_reference_particle()\n",
    "energy = reference_particle.get_total_energy()\n",
    "opts.beta = reference_particle.get_beta()\n",
    "opts.gamma = reference_particle.get_gamma()\n",
    "\n",
    "\n",
    "#================== Setting up the options =======================\n",
    "order = 1\n",
    "nsteps_per_element = 4\n",
    "\n",
    "\n",
    "opts.gridx = 32\n",
    "opts.gridy = 32\n",
    "opts.gridz = 1\n",
    "\n",
    "n_macro = opts.macro_particles\n",
    "\n",
    "nsteps = len(lattice_dict['t1_dQ03_1IO_82']['lattice'].get_elements())*nsteps_per_element\n",
    "opts.steps = nsteps\n",
    "\n",
    "#==================== Set up space charge solver ==========================\n",
    "\n",
    "requested_stepper = opts.stepper\n",
    "if opts.spacecharge:\n",
    "    \n",
    "    solver = opts.solver\n",
    "    # space charge only works with the split operator stepper, or soelements \n",
    "    if (requested_stepper != \"splitoperator\") and (requested_stepper != \"soelements\"):\n",
    "        requested_stepper = \"soelements\"\n",
    "        print \"Requested stepper changed to soelements for space charge\"\n",
    "\n",
    "    #force these\n",
    "    gridx = 32\n",
    "    gridy = 32\n",
    "    gridz = 1\n",
    "    grid = [gridx, gridy, gridz]\n",
    "\n",
    "    print >>logger, \"grid: \", grid\n",
    "\n",
    "    #opts.comm_divide = None\n",
    "    if opts.comm_divide:\n",
    "        sc_comm = synergia.utils.Commxx_divider(opts.comm_divide, False)\n",
    "    else:\n",
    "        sc_comm = synergia.utils.Commxx(True)\n",
    "\n",
    "    #sc_comm = synergia.utils.Commxx(True)\n",
    "    if solver == \"2dopen-hockney\":\n",
    "        coll_operator = synergia.collective.Space_charge_2d_open_hockney(sc_comm, grid)\n",
    "    elif solver == \"3dopen-hockney\":\n",
    "        # full signature for 3d_open_hockney constructor is\n",
    "        # comm, grid, long_kicks, z_periodic, period, grid_entire_period,\n",
    "        # nsigma\n",
    "\n",
    "        coll_operator = synergia.collective.Space_charge_3d_open_hockney(sc_comm, grid, opts.long_kicks, False, 0.0, False, opts.nsigma)\n",
    "    elif solver == \"2dbassetti-erskine\":\n",
    "        coll_operator = synergia.collective.Space_charge_2d_bassetti_erskine()\n",
    "    else:\n",
    "        raise RuntimeError, \"requested space charge operator %s invalid.  Must be either 2dopen-hockney or 3dopen-hockney\"%opts.solver\n",
    "\n",
    "    print \"Using space charge solver \", solver\n",
    "    print \"Grid: \", gridx, \" x \", gridy, \" x \", gridz\n",
    "\n",
    "else:\n",
    "    coll_operator = synergia.simulation.Dummy_collective_operator(\"stub\")\n",
    "    print \"No space charge solver used\"\n",
    "\n",
    "#opts.use_maps = 'none'\n",
    "#now set element type and construct stepper\n",
    "print \"use maps for: {}\".format(opts.use_maps)\n",
    "print \"requested_stepper: {}\".format(requested_stepper)\n",
    "\n",
    "        \n",
    "#================== Setting up the stepper and lattice simulator =======================\n",
    "\n",
    "for key in lattices.keys():\n",
    "    \n",
    "    current_lattice = lattice_dict[key]['lattice']\n",
    "    lattice_dict[key]['stepper'] = latticework.generate_stepper(current_lattice,coll_operator, opts)\n",
    "    lattice_dict[key]['lattice_simulator'] = lattice_dict[key]['stepper'].get_lattice_simulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'t3_dQ03_1IO_82'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d870eeebebcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Check that chef propagation is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlattice1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlattice_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't3_dQ03_1IO_82'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lattice'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlattice1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_string_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 't3_dQ03_1IO_82'"
     ]
    }
   ],
   "source": [
    "#Check that chef propagation is used\n",
    "lattice1 = lattice_dict['t3_dQ03_1IO_82']['lattice']\n",
    "for elem in lattice1.get_elements():\n",
    "    print elem.get_string_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lattice_simulator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7fcf042a8123>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"For t1 v8.2 lattice:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlattice_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't1_dQ03_1IO_82'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lattice_simulator'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_both_tunes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"For t3 v8.2 lattice:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlattice_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't3_dQ03_1IO_82'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lattice_simulator'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_both_tunes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lattice_simulator'"
     ]
    }
   ],
   "source": [
    "print \"For t1 v8.2 lattice:\" + str(lattice_dict['t1_dQ03_1IO_82']['lattice_simulator'].get_both_tunes())\n",
    "print \"For t3 v8.2 lattice:\" + str(lattice_dict['t3_dQ03_1IO_82']['lattice_simulator'].get_both_tunes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opts.save = False\n",
    "opts.lattice_name = 'IOTA 8-2 1IO standard lattice'\n",
    "opts.lf_fns = ['beta_x','beta_y','D_x']\n",
    "opts.lattice = lattice_dict['t1_dQ03_1IO_82']['lattice']\n",
    "opts.lattice_simulator = lattice_dict['t1_dQ03_1IO_82']['lattice_simulator']\n",
    "lfplot.plot_sliced_lattice_functions(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Initial Lattice Parameters for Use in Warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initial twiss parameters from Synergia lattice\n",
    "lf_names = (\"beta_x\", \"alpha_x\", \"beta_y\", \"alpha_y\", \n",
    "        \"psi_x\", \"psi_y\",\"D_x\", \"Dprime_x\", \"D_y\", \"Dprime_y\")\n",
    "lf = {}\n",
    "for lfname in lf_names:\n",
    "    lf[lfname] = np.empty([0,0])\n",
    "\n",
    "for element in lattice_dict['t1_dQ03_1IO_82']['lattice'].get_elements():\n",
    "    lattice_functions = lattice_dict['t1_dQ03_1IO_82']['lattice_simulator'].get_lattice_functions(element)\n",
    "    for lfname in lf_names:\n",
    "        lf[lfname] = np.append(lf[lfname],getattr(lattice_functions,lfname))\n",
    "\n",
    "test11 = lattice_dict['t1_dQ03_1IO_82']['lattice']\n",
    "test11.as_string\n",
    "        \n",
    "print \"Initial parameters for lattice: \\nbetax = %s\\nbetay = %s\\nalphax = %s\\nalphay = %s\" % (lf['beta_x'][0], lf['beta_y'][0],lf['alpha_x'][0],lf['alpha_y'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the beam and propagate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opts.lattice = lattice_dict['t3_dQ03_1IO_82']['lattice']\n",
    "opts.lattice_simulator = lattice_dict['t3_dQ03_1IO_82']['lattice_simulator']\n",
    "opts.stepper = lattice_dict['t3_dQ03_1IO_82']['stepper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dQ_SC_Gauss(N,emit,ref):\n",
    "    '''Return dQsc if given normalized emittance'''\n",
    "    r0 = 1.54e-18 #classical proton radius\n",
    "    bet = ref.get_beta()\n",
    "    gam = ref.get_gamma()\n",
    "    \n",
    "    dQ = -1.*r0*N/(4*np.pi*emit*bet*gam*gam)\n",
    "    #print gam*gam*gam\n",
    "    #print bet*bet\n",
    "    \n",
    "    return dQ\n",
    "\n",
    "def dQ_SC_Gauss_2(N,emit,bet,gam):\n",
    "    '''Return dQsc if given normalized emittance'''\n",
    "    r0 = 1.54e-18 #classical proton radius\n",
    "    \n",
    "    dQ = -1.*r0*N/(4*np.pi*emit*bet*gam*gam)\n",
    "    #print gam*gam*gam\n",
    "    #print bet*bet\n",
    "    \n",
    "    return dQ\n",
    "\n",
    "\n",
    "def dQ_SC_KV(N,emit,ref):\n",
    "    '''Return dQsc for a KV beam if given normalized emittance (2x that of Gaussian)'''\n",
    "    r0 = 1.54e-18 #classical proton radius\n",
    "    bet = ref.get_beta()\n",
    "    gam = ref.get_gamma()\n",
    "    \n",
    "    dQ = -1.*r0*N/(2*np.pi*emit*bet*gam*gam)\n",
    "    #print gam*gam*gam\n",
    "    #print bet*bet\n",
    "    \n",
    "    return dQ\n",
    "\n",
    "\n",
    "g_emit = 20.e-6 #TOTAL geometric emittance according to Sasha\n",
    "n_emit = basic_calcs.calc_normalized_emittance(g_emit,opts.beta,opts.gamma)\n",
    "\n",
    "current = 3.78 / 3.30 *1.e-3 #mA of current \n",
    "l_IOTA = 39.968229715800064 #length of lattice\n",
    "rp_perlength = current/(reference_particle.get_beta()*scipy.constants.c*scipy.constants.e)\n",
    "n_particles = rp_perlength*l_IOTA\n",
    "\n",
    "dQ = dQ_SC_Gauss(n_particles,n_emit,reference_particle) #calc dQsc_x\n",
    "dQ_KV = dQ_SC_KV(n_particles,n_emit,reference_particle) #calc dQsc_x\n",
    "\n",
    "print \"Assume a normalized total emittance of {} mm-mrad.\".format(n_emit*1.e6)\n",
    "print \"At {} mA current, # of protons filling ring is {:e} or {:e} p+/cm.\".format(current*1.e3,n_particles,n_particles/(100.*l_IOTA))\n",
    "print \"Corresponding space charge tune shift in x is {} for KV distribution\".format(dQ_KV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#======================= Now setup the bunch and other related options =====================\n",
    "tval = 0.1\n",
    "cval = 0.01\n",
    "\n",
    "opts.t = tval\n",
    "opts.c = cval\n",
    "opts.new_tune = 0.3\n",
    "opts.lnll = 1.8\n",
    "opts.nseg = 20\n",
    "vals = basic_calcs.get_base_nll(opts.nseg, opts.lnll, opts.new_tune, opts.t, opts.c)\n",
    "\n",
    "#specify vals for center of the section\n",
    "#opts.betae = vals[3]\n",
    "#opts.alphae = 0 #fixed 0 alpha for center\n",
    "#opts.beta0 = vals[3]\n",
    "\n",
    "#Get immediate beta value from lattice simulator and match to this instead\n",
    "new_vals = latticework.get_starting_lf(lattice_dict['t1_dQ03_1IO_82']['lattice_simulator'])\n",
    "opts.betae = new_vals[0]\n",
    "opts.alphae = 0\n",
    "opts.beta0 = new_vals[0]\n",
    "\n",
    "\n",
    "#opts.dpop = 0.4/100 #0.4% dpop\n",
    "dpop = 0.0\n",
    "opts.dpop = dpop #0.1% dpop\n",
    "\n",
    "rp_perlength = current/(opts.beta*constants.c*constants.e)\n",
    "bunch_length = opts.lattice.get_length() #effective bunch length is iota lattice length\n",
    "opts.real_particles = rp_perlength*bunch_length\n",
    "\n",
    "opts.emit = 0.5 * g_emit# USE THE GEOMETRIC EMITTANCE!!!!\n",
    "opts.emits = [opts.emit]\n",
    "\n",
    "\n",
    "if myrank == 0:\n",
    "    #construct a bunch and make sure to add longitudinal momentum variation\n",
    "    #particles = SemiGaussian6D.semiGaussianBeam6D(opts)\n",
    "    particles = EllipticBeam6D.toyEllipticalBeam6D(opts)#StandardBeam6D.toyKVBeam6D(opts)\n",
    "\n",
    "    for index in range(len(opts.emits)):\n",
    "        bunch = particles[index]\n",
    "        #initialH,initialI = elliptic_sp.calc_bunch_H(bunch,opts)\n",
    "        #bunch_mean = np.mean(initialH)\n",
    "        #bunch_std = np.std(initialH)\n",
    "        #bunch_var = (bunch_std/bunch_mean)*100\n",
    "        #print \"Constructed bunch with {} macroparticles, having mean H: {} and std: {}%\".format(opts.macro_particles, bunch_mean,bunch_var)\n",
    "        #now add longitudinal momentum variation\n",
    "        #For random samples with mean = 0, sigma = sigma, use sigma*np.random.randn(...)\n",
    "        #bunch[:,5] = opts.dpop*np.random.randn(1,len(bunch))\n",
    "        bunch[:,4] = bunch_length*(np.random.random(len(bunch)) -0.5) #center at 0\n",
    "        bunch[:,5] = opts.dpop*np.random.randn(1,len(bunch)) #set dp/p\n",
    "\n",
    "    np.savetxt('my_KV_bunch_82.txt',bunch)         #write the bunch to a text file\n",
    "\n",
    "\n",
    "bucket_length = bunch_length\n",
    "particles_file = 'my_KV_bunch_82.txt'\n",
    "myBunch = read_bunch.read_bunch(particles_file, reference_particle, opts.real_particles, bucket_length, comm)\n",
    "\n",
    "# generated longitudinal coordinate is z position (beta*c*dt) but Synergia uses\n",
    "# c*dt.  Divide by beta to get c*dt.\n",
    "local_particles = myBunch.get_local_particles()\n",
    "local_particles[:,4] /= opts.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pltbunch.plot_bunch(myBunch)\n",
    "pltbunch.plot_long(myBunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basic_calcs.calc_properties(myBunch,reference_particle)\n",
    "\n",
    "initialH,initialI = elliptic_sp.calc_bunch_H(myBunch,opts)\n",
    "bunch_mean = np.mean(initialH)\n",
    "bunch_std = np.std(initialH)\n",
    "print \"\\nInitial H = %s\\nstd of H = %s\" % (bunch_mean,bunch_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputdir = '8-2_dQm0p03_NLL'\n",
    "\n",
    "opts.output_dir = outputdir\n",
    "workflow.make_path(outputdir)\n",
    "\n",
    "bunch_simulator = synergia.simulation.Bunch_simulator(myBunch)\n",
    "\n",
    "#basic diagnostics - PER STEP\n",
    "basicdiag = synergia.bunch.Diagnostics_basic(\"basic.h5\", opts.output_dir)\n",
    "bunch_simulator.add_per_step(basicdiag)\n",
    "\n",
    "#include full diagnostics\n",
    "fulldiag = synergia.bunch.Diagnostics_full2(\"full.h5\", opts.output_dir)\n",
    "bunch_simulator.add_per_turn(fulldiag)\n",
    "\n",
    "#particle diagnostics - PER TURN\n",
    "opts.turnsPerDiag = 1\n",
    "particlediag = synergia.bunch.Diagnostics_particles(\"particles.h5\",0,0,opts.output_dir)\n",
    "bunch_simulator.add_per_turn(particlediag, opts.turnsPerDiag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opts.turns = 1044\n",
    "opts.checkpointperiod = 10\n",
    "opts.maxturns = opts.turns+1\n",
    "\n",
    "propagator = synergia.simulation.Propagator(opts.stepper)\n",
    "propagator.set_checkpoint_period(opts.checkpointperiod)\n",
    "propagator.propagate(bunch_simulator,opts.turns, opts.maxturns,opts.verbosity)\n",
    "\n",
    "workflow.cleanup(opts.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### Analysis - Beam Envelopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from base_diagnostics import diagplot2\n",
    "\n",
    "opts.output_dir = outputdir\n",
    "opts.relpath = opts.output_dir\n",
    "\n",
    "opts.inputfile = opts.output_dir + '/basic.h5'\n",
    "opts.plots = ['x_std', 'y_std']\n",
    "plotVals = diagplot2.getPlotVals(opts.inputfile, opts.plots)\n",
    "\n",
    "#define specific value arrays\n",
    "xmaster = plotVals['s']\n",
    "xstd = plotVals['x_std']\n",
    "ystd = plotVals['y_std']\n",
    "\n",
    "interval = opts.steps \n",
    "xstd_0 = xstd[:interval]\n",
    "xstd_2 = xstd[1*interval:2*interval]\n",
    "xstd_end = xstd[-1*interval:] \n",
    "\n",
    "ystd_0 = ystd[:interval]\n",
    "ystd_end = ystd[-1*interval:] \n",
    "\n",
    "#We can use the same s value for each plot\n",
    "sval_0 = xmaster[:interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = plt.gca()\n",
    "ax.plot(sval_0,xstd_0*1.e3,'b-',alpha=0.7, label='Turn 1') #plot the 1st turn\n",
    "ax.plot(sval_0,xstd_2*1.e3,'g-',alpha=0.7, label='Turn %s' % opts.turns) #plot the 1st turn\n",
    "axtitle = \"Beam envelope evolution - $\\sigma_x$ over %s turns\" % opts.turns\n",
    "ax.set_title(axtitle, y = 1.02, fontsize = 18)  \n",
    "ax.set_xlabel(\"s [m]\",fontsize=14)\n",
    "ax.set_ylabel(\"rms beam size $\\sigma_x$ [mm]\",fontsize=14)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.set_xlim([0,opts.lattice.get_length()])\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "#fig.savefig(sv_title,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = plt.gca()\n",
    "ax.plot(sval_0,ystd_0*1.e3,'b-',alpha=0.7, label='Turn 1') #plot the 1st turn\n",
    "ax.plot(sval_0,ystd_end*1.e3,'g-',alpha=0.7, label='Turn %s' % opts.turns) #plot the 1st turn\n",
    "axtitle = \"Beam envelope evolution - $\\sigma_y$ over %s turns\" % opts.turns\n",
    "ax.set_title(axtitle, y = 1.02, fontsize = 18)  \n",
    "ax.set_xlabel(\"s [m]\",fontsize=14)\n",
    "ax.set_ylabel(\"rms beam size $\\sigma_y$ [mm]\",fontsize=14)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.set_xlim([0,opts.lattice.get_length()])\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "#fig.savefig(sv_title,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis - Bunch Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pltbunch.plot_bunch(myBunch)\n",
    "pltbunch.plot_long(myBunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basic_calcs.calc_properties(myBunch,reference_particle)\n",
    "initialH,initialI = elliptic_sp.calc_bunch_H(myBunch,opts)\n",
    "bunch_mean = np.mean(initialH)\n",
    "bunch_std = np.std(initialH)\n",
    "print bunch_mean, bunch_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print opts.bunch_file, opts.relpath\n",
    "print len(elliptic_sp.get_lost_particle_list(opts))\n",
    "opts.plots = ['x','px']\n",
    "opts.plot_lost = False\n",
    "opts.lost = True\n",
    "opts.num = 1\n",
    "opts.scale = 2\n",
    "opts.lattice_name = 'IOTA 8-2 dQ=-0.1 Correction'\n",
    "opts.save = False\n",
    "elliptic_sp.plot_Poincare(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = elliptic_sp.get_file_list(opts)\n",
    "twiss = twiss = elliptic_sp.get_toy_twiss(opts)\n",
    "\n",
    "rms_table = []\n",
    "\n",
    "\n",
    "for inputfile in files:\n",
    "    fn = inputfile[-7:-3]\n",
    "    \n",
    "    parts = elliptic_sp.get_particles(inputfile)[1]\n",
    "    header= {}\n",
    "    header['s_val'] = 0.\n",
    "    #norm_coords = elliptic_sp.normalized_coordinates(header, part_array, twiss)\n",
    "    \n",
    "    x_rms = basic_calcs.get_rms_envelope('x',parts)\n",
    "    y_rms = basic_calcs.get_rms_envelope('y',parts)\n",
    "    \n",
    "    rms_table.append((int(fn),x_rms,y_rms))\n",
    "    \n",
    "    #print \"File {} : xrms = {:.2f} mm and yrms = {:.2f} mm\".format(fn, x_rms*1.e3, y_rms*1.e3)\n",
    "    \n",
    "rms_array = np.asarray(rms_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_turns = rms_array.shape[0]-1\n",
    "\n",
    "fig = plt.figure(figsize = (8,6))\n",
    "plt.subplot(2,1,1)\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(rms_array[:,0],rms_array[:,1]*1000, label = '$x_{rms}$')\n",
    "\n",
    "axtitle = \"Beam envelope evolution over {} turns with KV beam\".format(num_turns)\n",
    "ax.set_title(axtitle, y = 1.02, fontsize = 18)  \n",
    "ax.set_xlabel(\"Turn Number\",fontsize=14)\n",
    "ax.set_ylabel(\"rms beam size [mm]\",fontsize=14)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.set_xlim([0,num_turns])\n",
    "ax.legend(loc=2)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "bx = plt.gca()\n",
    "bx.plot(rms_array[:,0],rms_array[:,2]*1000, label = '$y_{rms}$')\n",
    "bxtitle = \"Beam envelope evolution over {} turns with KV beam\".format(num_turns)\n",
    "bx.set_title(bxtitle, y = 1.02, fontsize = 18)  \n",
    "bx.set_xlabel(\"Turn Number\",fontsize=14)\n",
    "bx.set_ylabel(\"rms beam size [mm]\",fontsize=14)\n",
    "bx.tick_params(axis='x', labelsize=14)\n",
    "bx.tick_params(axis='y', labelsize=14)\n",
    "bx.set_xlim([0,num_turns])\n",
    "bx.legend(loc=2)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis - Tune depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_particle_coords(ID, num, opts):\n",
    "    '''Return particle (human) coordinates for particle with ID over first num turns'''\n",
    "    \n",
    "    files = elliptic_sp.get_file_list(opts)\n",
    "    twiss = elliptic_sp.get_toy_twiss(opts)\n",
    "\n",
    "    part_array = []\n",
    "\n",
    "    for index,outfile in enumerate(files[:num]):\n",
    "        \n",
    "        #if index%20 == 0:\n",
    "            #print \"Grabbing particle {} coordinates for file {}\".format(ID,index)\n",
    "    \n",
    "        particle = elliptic_sp.get_one_particle(outfile,ID)\n",
    "    \n",
    "        if index ==0:\n",
    "            part_array = particle\n",
    "        else:\n",
    "            part_array = np.vstack([part_array,particle])\n",
    "            \n",
    "    return part_array\n",
    "\n",
    "def estimate_tune(x_c, t_s = 1):\n",
    "    '''Estimate the tune using an FFT of particle coordinates'''\n",
    "    \n",
    "    num_used = len(x_c[t_s:])\n",
    "    tv = np.arange(num_used)*1.0/num_used\n",
    "    sp = np.fft.fft(x_c[t_s:])\n",
    "    #plt.plot(tv,sp.real)\n",
    "\n",
    "\n",
    "    smax = np.max(sp.real)\n",
    "    m_ind = np.where(sp.real == smax)\n",
    "    Q_guess =m_ind[0][0]*1./num_used\n",
    "    if Q_guess > 0.5:\n",
    "        Q_calc = 1.- Q_guess\n",
    "    else:\n",
    "        Q_calc = Q_guess\n",
    "    #print \"Maximum is at {}\".format(Q_calc)\n",
    "    return Q_calc\n",
    "\n",
    "def estimate_tune_unwrap(x_c,px_c,t_s):\n",
    "    '''Estimate the tune using a phase unwrap algorithm\n",
    "    \n",
    "    Inputs:\n",
    "        -x_c = normalized spatial coordinate\n",
    "        -px_c = normalized momentum cooridnate\n",
    "        -t_s = starting turn value from the array of coordinates\n",
    "    \n",
    "    '''\n",
    "    ang_norm = []\n",
    "    for x,y in zip(x_c,px_c):\n",
    "        if x > 0 and y > 0: #quandrant I\n",
    "            ang_norm.append(np.arctan(y/x))\n",
    "        elif x < 0 and y > 0: #quandrant II\n",
    "            ang_norm.append(0.5*np.pi + (0.5*np.pi - np.abs(np.arctan(y/x))))\n",
    "        elif x < 0 and y < 0: #quadrant III\n",
    "            ang_norm.append(np.pi + np.abs(np.arctan(y/x)))\n",
    "        else: #quadrant IV\n",
    "            ang_norm.append(1.5*np.pi + (0.5*np.pi - np.abs(np.arctan(y/x))))\n",
    "\n",
    "    #take diference between elements\n",
    "    diff_ang = np.ediff1d(ang_norm)\n",
    "\n",
    "    #adjust for wrapping by replacing positive values with 2pi-val\n",
    "    dff_adjust = []\n",
    "    for val in diff_ang:\n",
    "        if val > 0:\n",
    "            val = val - 2*np.pi\n",
    "        dff_adjust.append(val)\n",
    "\n",
    "    #now look at the last 50-80 turns\n",
    "    #np.mean(dff_adjust[30:])\n",
    "    t_s = 0\n",
    "    meantune = -1*(np.mean(dff_adjust[t_s:]))/(2*np.pi)\n",
    "    #print \"{} is the mean tune for particle 0 after turn {}\".format(meantune,t_s)\n",
    "    return meantune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_some_particles(inputfile, ID=np.arange(100)):\n",
    "    '''\n",
    "    Reads an input file and returns a several particles' coordinates specified by particle ID.\n",
    "    \n",
    "    Arguments:\n",
    "        inputfile (str): path to a .h5 file containing particle diagnostics.\n",
    "        ID (Optional[list]): list of particle ID for the one particle to get. Defaults to [1:100]\n",
    "        \n",
    "    Returns:\n",
    "        part_vals (ndArray): array of particle data [x, x', y, y', cdt, dp, ID] for each particle\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    f = tables.openFile(inputfile, 'r')\n",
    "    particles = f.root.particles.read()\n",
    "    \n",
    "    #get appropriate reference properties from file root\n",
    "    npart = particles.shape[0]\n",
    "    mass = f.root.mass[()]\n",
    "    p_ref = f.root.pz[()]\n",
    "    sn = f.root.s_n[()] #period length\n",
    "    tn = f.root.tlen[()] #cumulative tracked length for this file\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    #ID = np.arange(5000)\n",
    "    \n",
    "    header = dict()\n",
    "    header['n_part'] = npart\n",
    "    header['mass'] = mass\n",
    "    header['p_ref'] = p_ref\n",
    "    header['s_val'] = sn\n",
    "    header['t_len'] = tn\n",
    "    \n",
    "    part_vals = []\n",
    "    \n",
    "    #separate lost particles\n",
    "    for particle in particles:\n",
    "        val = particle[6]\n",
    "        if val in ID:\n",
    "            part_vals.append(particle)\n",
    "            \n",
    "    return np.asarray(part_vals)\n",
    "\n",
    "\n",
    "def get_n_particles(inputfile, ID=100):\n",
    "    '''\n",
    "    Reads an input file and returns a several particles' coordinates specified by particle ID.\n",
    "    \n",
    "    Arguments:\n",
    "        inputfile (str): path to a .h5 file containing particle diagnostics.\n",
    "        ID (Optional[list]): list of particle ID for the one particle to get. Defaults to [1:100]\n",
    "        \n",
    "    Returns:\n",
    "        part_vals (ndArray): array of particle data [x, x', y, y', cdt, dp, ID] for each particle\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    f = tables.openFile(inputfile, 'r')\n",
    "    particles = f.root.particles.read()\n",
    "    \n",
    "    #get appropriate reference properties from file root\n",
    "    npart = particles.shape[0]\n",
    "    mass = f.root.mass[()]\n",
    "    p_ref = f.root.pz[()]\n",
    "    sn = f.root.s_n[()] #period length\n",
    "    tn = f.root.tlen[()] #cumulative tracked length for this file\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    #ID = np.arange(5000)\n",
    "    \n",
    "    header = dict()\n",
    "    header['n_part'] = npart\n",
    "    header['mass'] = mass\n",
    "    header['p_ref'] = p_ref\n",
    "    header['s_val'] = sn\n",
    "    header['t_len'] = tn\n",
    "    \n",
    "    part_vals = []\n",
    "    \n",
    "    #separate lost particles\n",
    "    for particle in particles:\n",
    "        if len(part_vals) < ID:\n",
    "            part_vals.append(particle)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return np.asarray(part_vals)\n",
    "\n",
    "num_t = 1044\n",
    "tune_array = []\n",
    "part_array = []\n",
    "\n",
    "twiss = twiss = elliptic_sp.get_toy_twiss(opts)\n",
    "header= {}\n",
    "header['s_val'] = 0.\n",
    "files = elliptic_sp.get_file_list(opts)[:num_t]\n",
    "\n",
    "IDlist = get_n_particles(files[-1],ID=400)[:,6]\n",
    "\n",
    "\n",
    "for index,inputfile in enumerate(files):\n",
    "    #get all of the particles specifed by IDlist\n",
    "    p_A = get_some_particles(inputfile,IDlist)\n",
    "    norm_coords = elliptic_sp.normalized_coordinates(header, p_A, twiss)\n",
    "    if len(part_array) == 0:\n",
    "        #if empty, then replace with norm_coords - first turn\n",
    "        part_array = norm_coords\n",
    "    else:\n",
    "        part_array = np.vstack((part_array,norm_coords))\n",
    "    \n",
    "    \n",
    "#reshape array to be indexable by ID number     \n",
    "new_PA = part_array.reshape(num_t,len(IDlist),4)\n",
    "\n",
    "x_tune_array = []\n",
    "y_tune_array = []\n",
    "for ID in range(len(IDlist)):\n",
    "    x_tune_array.append(estimate_tune(new_PA[:,ID,0]))\n",
    "    y_tune_array.append(estimate_tune(new_PA[:,ID,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (6,6))\n",
    "ax = fig.gca()\n",
    "\n",
    "binwidth=0.005\n",
    "xbins = np.arange(min(x_tune_array), max(x_tune_array) + binwidth, binwidth)\n",
    "ybins = np.arange(min(y_tune_array), max(y_tune_array) + binwidth, binwidth)\n",
    "\n",
    "ax.hist(x_tune_array,xbins,label='$\\\\nu_x$')\n",
    "\n",
    "\n",
    "bx = fig.gca()\n",
    "bx.hist(y_tune_array,ybins,label='$\\\\nu_y$')\n",
    "\n",
    "bx.legend()\n",
    "bx.set_title('x and y tunes')\n",
    "bx.set_xlabel('tune')\n",
    "bx.set_ylabel('population')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - Invariants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Collect particles, sort and return array of invariants over all particles and turns\n",
    "def lostlist(directory,npart):\n",
    "    lostlist = []\n",
    "    lastfile = sorted(os.listdir(directory))[-1]\n",
    "    print lastfile\n",
    "    header, bunchIn = elliptic_sp.get_particles(directory + '/' + lastfile)\n",
    "    print bunchIn.shape\n",
    "    for i in range(npart):\n",
    "        if not np.any(bunchIn[:,6] == i):\n",
    "            lostlist.append(i)\n",
    "            \n",
    "    return lostlist\n",
    "\n",
    "def get_invariants(directory,npart):\n",
    "    Harray = []\n",
    "    Iarray = []\n",
    "    lostParts = lostlist(directory,npart)\n",
    "\n",
    "    for bunchFile in sorted(os.listdir(directory)):\n",
    "        if bunchFile.endswith('.h5') and bunchFile.find('particles')!= -1:\n",
    "            header, bunchIn = elliptic_sp.get_particles(directory + '/' + bunchFile)\n",
    "            \n",
    "            for lost in lostParts:\n",
    "                rowlost = np.where(bunchIn[:,6] == lost)[0]\n",
    "                try:\n",
    "                    rowval = rowlost[0]\n",
    "                except IndexError:\n",
    "                    rowval = None\n",
    "                if rowval:\n",
    "                    bunchIn = np.delete(bunchIn,rowval,0)\n",
    "\n",
    "                rowval = None\n",
    "                \n",
    "            sBunch = bunchIn[np.argsort(bunchIn[:,6])]\n",
    "            Hval,Ival = elliptic_sp.calc_bunch_H(sBunch,opts)\n",
    "            Harray.append(Hval)\n",
    "            Iarray.append(Ival)\n",
    "            \n",
    "    return np.transpose(np.array(Harray)),np.transpose(np.array(Iarray))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Hinv, Iinv = get_invariants('8-2_dQm0p1_NLL',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figc = plt.figure(figsize=(12,6))\n",
    "cx = plt.gca()\n",
    "cx.plot(np.arange(opts.turns+1),Hinv[0,:]*1e6,'-b')\n",
    "cx.plot(np.arange(opts.turns+1),Hinv[50,:]*1e6,'-r')\n",
    "cx.plot(np.arange(opts.turns+1),Hinv[4242,:]*1e6,'-g')\n",
    "cx.set_xlim(-1,opts.turns+2)\n",
    "cx.set_xlabel(\"Turn Number\",fontsize=14)\n",
    "cx.set_ylabel(\"H (mm-mrad)\")\n",
    "cx.set_title(\"H-Invariant (for three particles)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figc = plt.figure(figsize=(12,6))\n",
    "cx = plt.gca()\n",
    "cx.plot(np.arange(opts.turns+1),[np.average(Hinv[:,i])*1e6 for i in range(Hinv.shape[1])],'-')\n",
    "cx.set_xlim(-1,opts.turns+2)\n",
    "#cx.set_ylim(9.5,10.5)\n",
    "cx.set_xlabel(\"Turn Number\",fontsize=14)\n",
    "cx.set_ylabel( \"<H> (mm-mrad)\")\n",
    "cx.set_title(\"Average of H-Invariant\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figc = plt.figure(figsize=(12,6))\n",
    "cx = plt.gca()\n",
    "cx.plot(np.arange(opts.turns+1),[np.std(Hinv[:,i])*1e6 for i in range(Hinv.shape[1])],'-')\n",
    "cx.set_xlim(-10,opts.turns+2)\n",
    "cx.set_xlabel(\"Turn Number\",fontsize=14)\n",
    "cx.set_ylabel(\"$\\sigma_H$ (mm-mrad)\")\n",
    "cx.set_title(\"standard deviation of the H-invariant\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figc = plt.figure(figsize=(12,6))\n",
    "cx = plt.gca()\n",
    "cx.plot(np.arange(opts.turns+1),Iinv[0,:]*1e6,'-b')\n",
    "cx.plot(np.arange(opts.turns+1),Iinv[50,:]*1e6,'-r')\n",
    "cx.plot(np.arange(opts.turns+1),Iinv[4242,:]*1e6,'-g')\n",
    "cx.set_xlim(-1,1+opts.turns)\n",
    "cx.set_xlabel(\"Turn Number\",fontsize=14)\n",
    "cx.set_ylabel(\"I (mm-mrad)\")\n",
    "cx.set_title(\"Single Particle I-invariant (for three particles)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figc = plt.figure(figsize=(12,6))\n",
    "cx = plt.gca()\n",
    "cx.plot(np.arange(opts.turns+1),[np.std(Iinv[:,i])*1e6 for i in range(Iinv.shape[1])],'-',c='g')\n",
    "cx.set_xlim(-10,opts.turns+2)\n",
    "cx.set_xlabel(\"Turn Number\",fontsize=14)\n",
    "cx.set_ylabel(\"$\\sigma_I$ (mm-mrad)\")\n",
    "cx.set_title(\"standard deviation of I-invariant\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - Additional Invariant Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.scatter(Hinv[60,:]*1e6,Iinv[60,:]*1e9,c=np.arange(len(Hinv[0,:])))\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"H-Invariant\")\n",
    "plt.ylabel(\"I-Invariant\")\n",
    "plt.title(\"Single Particle I vs H over %s turns\" % opts.turns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "p = ax.scatter(Hinv[10,:]*1e6,Iinv[10,:]*1e9,np.arange(len(Hinv[0,:])),c=np.arange(len(Hinv[0,:])))\n",
    "ax.set_xlabel('H-Invariant')\n",
    "ax.set_ylabel('I-Invariant')\n",
    "ax.set_zlabel('Turn')\n",
    "\n",
    "fig.colorbar(p)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.hist2d(Hinv[:,1000]*1e6,Iinv[:,1000]*1e6,128)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"H-Invariant\")\n",
    "plt.ylabel(\"I-Invariant\")\n",
    "plt.title(\"2D Histogram of I vs H for all particles on turn 1000\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "turns = np.arange(0,1000,80)\n",
    "colors = cm.rainbow(np.linspace(0,1,len(turns)))\n",
    "for ni, c in zip(turns,colors):\n",
    "    p = ax.scatter(Hinv[100:800,ni]*1e6,Iinv[100:800,ni]*1e6,ni,color=c)\n",
    "ax.set_xlabel('H')\n",
    "ax.set_ylabel('I')\n",
    "ax.set_zlabel('Turn')\n",
    "#fig.colorbar(p)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to store all particle output data to array, sorted, with lost particles removed from all turns\n",
    "# array: Array[turnNumber,particleID,dimension]\n",
    "def get_all_turns(directory,npart):\n",
    "    turn = []\n",
    "    lostParts = lostlist(directory,npart)\n",
    "\n",
    "    for bunchFile in sorted(os.listdir(directory)):\n",
    "        if bunchFile.endswith('.h5') and bunchFile.find('particles')!= -1:\n",
    "            header, bunchIn = elliptic_sp.get_particles(directory + '/' + bunchFile)\n",
    "            \n",
    "            for lost in lostParts:\n",
    "                rowlost = np.where(bunchIn[:,6] == lost)[0]\n",
    "                try:\n",
    "                    rowval = rowlost[0]\n",
    "                except IndexError:\n",
    "                    rowval = None\n",
    "                if rowval:\n",
    "                    bunchIn = np.delete(bunchIn,rowval,0)\n",
    "\n",
    "                rowval = None\n",
    "                \n",
    "            sBunch = bunchIn[np.argsort(bunchIn[:,6])]\n",
    "            turn.append(sBunch)\n",
    "    return np.array(turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allTurns = get_all_turns('8-2_dQm0p1_NLL',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print allTurns.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(np.arange(allTurns.shape[0]),allTurns[:,4242,5])\n",
    "plt.xlabel(\"Turn\")\n",
    "plt.ylabel(\"dp/p\")\n",
    "plt.title(\"dp/p vs turn for a single particles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figc = plt.figure(figsize=(12,6))\n",
    "cx = plt.gca()\n",
    "cx.plot(np.arange(opts.turns+1),[np.average(allTurns[i,:,5]) for i in range(allTurns.shape[0])],'-')\n",
    "cx.set_xlim(-1,opts.turns+2)\n",
    "#cx.set_ylim(9.5,10.5)\n",
    "cx.set_xlabel(\"Turn\",fontsize=14)\n",
    "cx.set_ylabel( \"Mean dp/p \")\n",
    "cx.set_title(\"Average of dp/p\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
